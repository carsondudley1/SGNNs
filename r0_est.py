# -*- coding: utf-8 -*-
"""R0_est.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19KLqdB1m_GIfettczPgdIGrQQVrmSF66
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import Tuple, List
import random
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm.auto import tqdm
import os

class SIRSimulator:
    def __init__(self):
        pass  # Remove default population size

    def simulate(self, beta: float, gamma: float, population_size: int = 10000, num_days: int = 30) -> Tuple[List[int], float]:
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0
        daily_new_cases = [1]

        for _ in range(num_days - 1):
            new_infections = np.random.binomial(S, 1 - np.exp(-beta * I / self.N))
            recoveries = np.random.binomial(I, 1 - np.exp(-gamma))

            S -= new_infections
            I = I + new_infections - recoveries
            R += recoveries

            daily_new_cases.append(new_infections)

        R0 = beta/gamma
        return daily_new_cases, R0

class SEIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, sigma: float, gamma: float, population_size: int = 10000, num_days: int = 30) -> Tuple[List[int], float]:
        self.N = population_size
        S = self.N - 1
        E = 0
        I = 1
        R = 0
        daily_new_cases = [1]

        for _ in range(num_days - 1):
            new_exposures = np.random.binomial(S, 1 - np.exp(-beta * I / self.N))
            new_infections = np.random.binomial(E, 1 - np.exp(-sigma))
            recoveries = np.random.binomial(I, 1 - np.exp(-gamma))

            S -= new_exposures
            E = E + new_exposures - new_infections
            I = I + new_infections - recoveries
            R += recoveries

            daily_new_cases.append(new_infections)

        R0 = beta/gamma
        return daily_new_cases, R0

class SIRSSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, omega: float, population_size: int = 10000, num_days: int = 30) -> Tuple[List[int], float]:
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0
        daily_new_cases = [1]

        for _ in range(num_days - 1):
            new_infections = np.random.binomial(S, 1 - np.exp(-beta * I / self.N))
            recoveries = np.random.binomial(I, 1 - np.exp(-gamma))
            waning = np.random.binomial(R, 1 - np.exp(-omega))

            S = S - new_infections + waning
            I = I + new_infections - recoveries
            R = R + recoveries - waning

            daily_new_cases.append(new_infections)

        R0 = beta/gamma
        return daily_new_cases, R0

class GillespieSIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, population_size: int = 10000,
                num_days: int = 30) -> Tuple[List[int], float]:
        """
        Simulate SIR model using Gillespie's SSA
        """
        # Initialize
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0

        # Time tracking
        t = 0
        current_day = 0
        daily_new_cases = [1]  # Initial case
        new_cases_today = 0

        # Store state history
        while t < num_days and I > 0:
            # Calculate propensities
            infection_rate = beta * S * I / self.N
            recovery_rate = gamma * I
            total_rate = infection_rate + recovery_rate

            if total_rate == 0:
                break

            # Time until next event
            dt = np.random.exponential(1/total_rate)
            t += dt

            # If we've moved to a new day
            while int(t) > current_day and current_day < num_days - 1:
                daily_new_cases.append(new_cases_today)
                new_cases_today = 0
                current_day += 1

            # Choose and execute next reaction
            if np.random.random() < infection_rate/total_rate:
                # Infection occurs
                S -= 1
                I += 1
                new_cases_today += 1
            else:
                # Recovery occurs
                I -= 1
                R += 1

        # Fill any remaining days with zeros
        while len(daily_new_cases) < num_days:
            daily_new_cases.append(0)

        R0 = beta/gamma
        return daily_new_cases, R0

class TauLeapingSIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, population_size: int = 10000,
                num_days: int = 30, tau: float = 0.1) -> Tuple[List[int], float]:
        """
        Simulate SIR model using tau-leaping
        """
        # Initialize
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0

        steps_per_day = int(1/tau)
        daily_new_cases = [1]  # Initial case
        new_cases_today = 0

        for day in range(num_days-1):
            new_cases_today = 0

            for _ in range(steps_per_day):
                # Calculate expected number of events in tau time
                infection_rate = beta * S * I / self.N
                recovery_rate = gamma * I

                # Generate number of events
                infections = np.random.poisson(infection_rate * tau)
                recoveries = np.random.poisson(recovery_rate * tau)

                # Bound by possible events
                infections = min(infections, S)
                recoveries = min(recoveries, I)

                # Update states
                S -= infections
                I = I + infections - recoveries
                R += recoveries

                new_cases_today += infections

            daily_new_cases.append(new_cases_today)

        R0 = beta/gamma
        return daily_new_cases, R0

class SIWRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, omega: float, delta: float,
                 water_contamination_rate: float, population_size: int = 10000,
                 num_days: int = 30) -> Tuple[List[int], float]:
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0
        W = 0  # Initial waterborne pathogen concentration
        daily_new_cases = [1]

        for _ in range(num_days - 1):
            # New infections from direct contact and waterborne pathogens
            new_infections = np.random.binomial(S, 1 - np.exp(-beta * I / self.N - delta * W))
            recoveries = np.random.binomial(I, 1 - np.exp(-gamma))
            waning = np.random.binomial(R, 1 - np.exp(-omega))

            # Update compartments
            S = S - new_infections + waning
            I = I + new_infections - recoveries
            R = R + recoveries - waning

            # Update waterborne pathogen concentration
            W = W + water_contamination_rate * new_infections - delta * W

            daily_new_cases.append(new_infections)

        R0 = (beta / gamma + water_contamination_rate / gamma)  # Basic reproduction number
        return daily_new_cases, R0

class VectorBorneSimulator:
    def __init__(self):
        pass

    def simulate(self, beta_h: float, beta_v: float, gamma_h: float,
                mu_v: float, human_population: int = 10000,
                vector_population: int = 50000, num_days: int = 30):
        self.Nh = human_population
        self.Nv = vector_population

        Sh = self.Nh - 1
        Ih = 1
        Rh = 0

        Sv = self.Nv
        Iv = 0

        daily_new_cases = [1]

        for _ in range(num_days - 1):
            new_human_infections = np.random.binomial(
                Sh, 1 - np.exp(-beta_h * Iv / self.Nv))
            new_vector_infections = np.random.binomial(
                Sv, 1 - np.exp(-beta_v * Ih / self.Nh))

            human_recoveries = np.random.binomial(Ih, 1 - np.exp(-gamma_h))
            vector_deaths = np.random.binomial(Iv, 1 - np.exp(-mu_v))

            Sh -= new_human_infections
            Ih += new_human_infections - human_recoveries
            Rh += human_recoveries

            Sv -= new_vector_infections
            Iv += new_vector_infections - vector_deaths

            daily_new_cases.append(new_human_infections)

        R0 = np.sqrt((beta_h * beta_v)/(gamma_h * mu_v))
        return daily_new_cases, R0

class GillespieSEIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, sigma: float, gamma: float,
                population_size: int = 10000, num_days: int = 30) -> Tuple[List[int], float]:
        """
        Simulate SEIR model using Gillespie's SSA
        """
        # Initialize
        self.N = population_size
        S = self.N - 1
        E = 0
        I = 1
        R = 0

        t = 0
        current_day = 0
        daily_new_cases = [1]  # Initial case
        new_cases_today = 0

        while t < num_days and (I > 0 or E > 0):
            # Calculate propensities
            exposure_rate = beta * S * I / self.N
            infection_rate = sigma * E
            recovery_rate = gamma * I
            total_rate = exposure_rate + infection_rate + recovery_rate

            if total_rate == 0:
                break

            dt = np.random.exponential(1/total_rate)
            t += dt

            # Record daily cases
            while int(t) > current_day and current_day < num_days - 1:
                daily_new_cases.append(new_cases_today)
                new_cases_today = 0
                current_day += 1

            # Choose next reaction
            rand = np.random.random()
            if rand < exposure_rate/total_rate:
                # New exposure
                S -= 1
                E += 1
            elif rand < (exposure_rate + infection_rate)/total_rate:
                # Exposed becomes infectious
                E -= 1
                I += 1
                new_cases_today += 1
            else:
                # Recovery
                I -= 1
                R += 1

        # Fill remaining days
        while len(daily_new_cases) < num_days:
            daily_new_cases.append(0)

        R0 = beta/gamma
        return daily_new_cases, R0

class GillespieAgeStructuredSimulator:
    def __init__(self):
        pass

    def simulate(self, contact_matrix, betas, gammas, population_sizes=[5000, 3000, 2000], num_days=30) -> Tuple[List[List[int]], float]:
        self.N = population_sizes
        n_groups = len(self.N)
        S = [n-1 if i==0 else n for i, n in enumerate(self.N)]
        I = [1 if i==0 else 0 for i in range(n_groups)]
        R = [0] * n_groups

        t = 0
        current_day = 0
        daily_new_cases = [[1 if i==0 else 0 for i in range(n_groups)]]

        while t < num_days and any(I):
            # Calculate propensities
            total_rate = 0
            infection_rates = []
            recovery_rates = []

            for i in range(n_groups):
                lambda_i = sum(contact_matrix[i][j] * betas[j] * I[j] / self.N[j] for j in range(n_groups))
                infection_rate = lambda_i * S[i]
                recovery_rate = gammas[i] * I[i]

                infection_rates.append(infection_rate)
                recovery_rates.append(recovery_rate)
                total_rate += infection_rate + recovery_rate

            if total_rate == 0:
                break

            dt = np.random.exponential(1/total_rate)
            t += dt

            while int(t) > current_day and current_day < num_days - 1:
                daily_new_cases.append([0] * n_groups)
                current_day += 1

            rand = np.random.random() * total_rate
            cumulative_rate = 0

            for i in range(n_groups):
                cumulative_rate += infection_rates[i]
                if rand < cumulative_rate:
                    S[i] -= 1
                    I[i] += 1
                    daily_new_cases[-1][i] += 1
                    break

                cumulative_rate += recovery_rates[i]
                if rand < cumulative_rate:
                    I[i] -= 1
                    R[i] += 1
                    break

        while len(daily_new_cases) < num_days:
            daily_new_cases.append([0] * n_groups)

        NGM = np.array([[contact_matrix[i][j] * betas[j] / gammas[i] for j in range(n_groups)] for i in range(n_groups)])
        R0 = max(abs(np.linalg.eigvals(NGM)))

        return daily_new_cases, R0

class GillespieSIWRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, omega: float, delta: float,
                 water_contamination_rate: float, population_size: int = 10000,
                 num_days: int = 30) -> Tuple[List[int], float]:
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0
        W = 0  # Initial waterborne pathogen concentration
        daily_new_cases = [1]

        t = 0
        current_day = 0
        new_cases_today = 0

        while t < num_days and I > 0:
            # Calculate propensities
            contact_infection_rate = beta * S * I / self.N
            water_infection_rate = delta * S * W
            recovery_rate = gamma * I
            waning_rate = omega * R
            total_rate = contact_infection_rate + water_infection_rate + recovery_rate + waning_rate

            if total_rate == 0:
                break

            dt = np.random.exponential(1/total_rate)
            t += dt

            while int(t) > current_day and current_day < num_days - 1:
                daily_new_cases.append(new_cases_today)
                new_cases_today = 0
                current_day += 1

            rand = np.random.random() * total_rate
            cumulative_rate = 0

            if rand < (cumulative_rate := cumulative_rate + contact_infection_rate):
                # Contact infection
                S -= 1
                I += 1
                new_cases_today += 1
            elif rand < (cumulative_rate := cumulative_rate + water_infection_rate):
                # Waterborne infection
                S -= 1
                I += 1
                new_cases_today += 1
            elif rand < (cumulative_rate := cumulative_rate + recovery_rate):
                # Recovery
                I -= 1
                R += 1
            else:
                # Waning immunity
                R -= 1
                S += 1

            # Update waterborne pathogen concentration
            W = W + water_contamination_rate * new_cases_today - delta * W

        while len(daily_new_cases) < num_days:
            daily_new_cases.append(0)

        R0 = beta / gamma
        return daily_new_cases, R0

class TauLeapingSEIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, sigma: float, gamma: float,
                population_size: int = 10000, num_days: int = 30,
                tau: float = 0.1) -> Tuple[List[int], float]:
        """
        Simulate SEIR model using tau-leaping
        """
        self.N = population_size
        S = self.N - 1
        E = 0
        I = 1
        R = 0

        steps_per_day = int(1/tau)
        daily_new_cases = [1]  # Initial case
        new_cases_today = 0

        for day in range(num_days-1):
            new_cases_today = 0

            for _ in range(steps_per_day):
                # Calculate rates
                exposure_rate = beta * S * I / self.N
                infection_rate = sigma * E
                recovery_rate = gamma * I

                # Generate events
                exposures = np.random.poisson(exposure_rate * tau)
                infections = np.random.poisson(infection_rate * tau)
                recoveries = np.random.poisson(recovery_rate * tau)

                # Bound by possible events
                exposures = min(exposures, S)
                infections = min(infections, E)
                recoveries = min(recoveries, I)

                # Update states
                S -= exposures
                E = E + exposures - infections
                I = I + infections - recoveries
                R += recoveries

                new_cases_today += infections

            daily_new_cases.append(new_cases_today)

        R0 = beta/gamma
        return daily_new_cases, R0

class TauLeapingSIRSSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, omega: float,
                population_size: int = 10000, num_days: int = 30,
                tau: float = 0.1) -> Tuple[List[int], float]:
        """
        Simulate SIRS model using tau-leaping
        """
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0

        steps_per_day = int(1/tau)
        daily_new_cases = [1]
        new_cases_today = 0

        for day in range(num_days-1):
            new_cases_today = 0

            for _ in range(steps_per_day):
                # Calculate rates
                infection_rate = beta * S * I / self.N
                recovery_rate = gamma * I
                waning_rate = omega * R

                # Generate events
                infections = np.random.poisson(infection_rate * tau)
                recoveries = np.random.poisson(recovery_rate * tau)
                wanings = np.random.poisson(waning_rate * tau)

                # Bound by possible events
                infections = min(infections, S)
                recoveries = min(recoveries, I)
                wanings = min(wanings, R)

                # Update states
                S = S - infections + wanings
                I = I + infections - recoveries
                R = R + recoveries - wanings

                new_cases_today += infections

            daily_new_cases.append(new_cases_today)

        R0 = beta/gamma
        return daily_new_cases, R0

class TauLeapingVectorBorneSimulator:
    def __init__(self):
        pass

    def simulate(self, beta_h: float, beta_v: float, gamma_h: float,
                mu_v: float, human_population: int = 10000,
                vector_population: int = 50000, num_days: int = 30,
                tau: float = 0.1) -> Tuple[List[int], float]:
        """
        Simulate vector-borne disease using tau-leaping
        """
        self.Nh = human_population
        self.Nv = vector_population

        Sh = self.Nh - 1
        Ih = 1
        Rh = 0

        Sv = self.Nv
        Iv = 0

        steps_per_day = int(1/tau)
        daily_new_cases = [1]
        new_cases_today = 0

        for day in range(num_days-1):
            new_cases_today = 0

            for _ in range(steps_per_day):
                # Calculate rates
                human_infection_rate = beta_h * Sh * Iv / self.Nv
                vector_infection_rate = beta_v * Sv * Ih / self.Nh
                human_recovery_rate = gamma_h * Ih
                vector_death_rate = mu_v * Iv

                # Generate events
                human_infections = np.random.poisson(human_infection_rate * tau)
                vector_infections = np.random.poisson(vector_infection_rate * tau)
                human_recoveries = np.random.poisson(human_recovery_rate * tau)
                vector_deaths = np.random.poisson(vector_death_rate * tau)

                # Bound by possible events
                human_infections = min(human_infections, Sh)
                vector_infections = min(vector_infections, Sv)
                human_recoveries = min(human_recoveries, Ih)
                vector_deaths = min(vector_deaths, Iv)

                # Update states
                Sh -= human_infections
                Ih += human_infections - human_recoveries
                Rh += human_recoveries

                Sv -= vector_infections
                Iv += vector_infections - vector_deaths

                new_cases_today += human_infections

            daily_new_cases.append(new_cases_today)

        R0 = np.sqrt((beta_h * beta_v)/(gamma_h * mu_v))
        return daily_new_cases, R0

class TauLeapingAgeStructuredSimulator:
    def __init__(self):
        pass

    def simulate(self, contact_matrix, betas, gammas,
                population_sizes=[5000, 3000, 2000], num_days=30,
                tau: float = 0.1) -> Tuple[List[List[int]], float]:
        """
        Simulate age-structured model using tau-leaping
        """
        self.N = population_sizes
        n_groups = len(self.N)
        S = [n-1 if i==0 else n for i, n in enumerate(self.N)]
        I = [1 if i==0 else 0 for i in range(n_groups)]
        R = [0] * n_groups

        steps_per_day = int(1/tau)
        daily_new_cases = [[1 if i==0 else 0 for i in range(n_groups)]]

        for day in range(num_days-1):
            new_cases_today = [0] * n_groups

            for _ in range(steps_per_day):
                infections = []
                recoveries = []

                # Calculate rates for each age group
                for i in range(n_groups):
                    lambda_i = sum(contact_matrix[i][j] * betas[j] * I[j] / self.N[j]
                                 for j in range(n_groups))
                    infection_rate = lambda_i * S[i]
                    recovery_rate = gammas[i] * I[i]

                    # Generate events
                    group_infections = np.random.poisson(infection_rate * tau)
                    group_recoveries = np.random.poisson(recovery_rate * tau)

                    # Bound by possible events
                    group_infections = min(group_infections, S[i])
                    group_recoveries = min(group_recoveries, I[i])

                    infections.append(group_infections)
                    recoveries.append(group_recoveries)

                # Update states
                for i in range(n_groups):
                    S[i] -= infections[i]
                    I[i] = I[i] + infections[i] - recoveries[i]
                    R[i] += recoveries[i]
                    new_cases_today[i] += infections[i]

            daily_new_cases.append(new_cases_today)

        NGM = np.array([[contact_matrix[i][j] * betas[j] / gammas[i]
                         for j in range(n_groups)] for i in range(n_groups)])
        R0 = max(abs(np.linalg.eigvals(NGM)))

        return daily_new_cases, R0

class TauLeapingSIWRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, omega: float, delta: float,
                water_contamination_rate: float, population_size: int = 10000,
                num_days: int = 30, tau: float = 0.1) -> Tuple[List[int], float]:
        """
        Simulate SIWR model using tau-leaping
        """
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0
        W = 0

        steps_per_day = int(1/tau)
        daily_new_cases = [1]
        new_cases_today = 0

        for day in range(num_days-1):
            new_cases_today = 0

            for _ in range(steps_per_day):
                # Calculate rates
                contact_infection_rate = beta * S * I / self.N
                water_infection_rate = delta * S * W
                recovery_rate = gamma * I
                waning_rate = omega * R

                # Generate events
                contact_infections = np.random.poisson(contact_infection_rate * tau)
                water_infections = np.random.poisson(water_infection_rate * tau)
                recoveries = np.random.poisson(recovery_rate * tau)
                wanings = np.random.poisson(waning_rate * tau)

                # Total infections cannot exceed susceptible population
                total_infections = min(contact_infections + water_infections, S)
                recoveries = min(recoveries, I)
                wanings = min(wanings, R)

                # Update states
                S = S - total_infections + wanings
                I = I + total_infections - recoveries
                R = R + recoveries - wanings

                # Update waterborne pathogen concentration
                W = W + water_contamination_rate * total_infections - delta * W

                new_cases_today += total_infections

            daily_new_cases.append(new_cases_today)

        R0 = (beta / gamma + water_contamination_rate / gamma)
        return daily_new_cases, R0

class GillespieSIRSSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, omega: float,
                population_size: int = 10000, num_days: int = 30) -> Tuple[List[int], float]:
        """
        Simulate SIRS model using Gillespie's SSA
        """
        # Initialize
        self.N = population_size
        S = self.N - 1
        I = 1
        R = 0

        t = 0
        current_day = 0
        daily_new_cases = [1]
        new_cases_today = 0

        while t < num_days and I > 0:
            # Calculate propensities
            infection_rate = beta * S * I / self.N
            recovery_rate = gamma * I
            waning_rate = omega * R
            total_rate = infection_rate + recovery_rate + waning_rate

            if total_rate == 0:
                break

            dt = np.random.exponential(1/total_rate)
            t += dt

            while int(t) > current_day and current_day < num_days - 1:
                daily_new_cases.append(new_cases_today)
                new_cases_today = 0
                current_day += 1

            rand = np.random.random()
            if rand < infection_rate/total_rate:
                # New infection
                S -= 1
                I += 1
                new_cases_today += 1
            elif rand < (infection_rate + recovery_rate)/total_rate:
                # Recovery
                I -= 1
                R += 1
            else:
                # Waning immunity
                R -= 1
                S += 1

        while len(daily_new_cases) < num_days:
            daily_new_cases.append(0)

        R0 = beta/gamma
        return daily_new_cases, R0

class NetworkSIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, population_size: int = 10000,
                num_days: int = 30, network_type: str = 'random',
                avg_connections: int = 10) -> Tuple[List[int], float]:
        """
        Simulate SIR on a contact network

        Args:
            beta: transmission rate per contact
            gamma: recovery rate
            population_size: total population
            num_days: simulation duration
            network_type: 'random', 'scale_free', or 'small_world'
            avg_connections: average number of connections per individual
        """
        import networkx as nx

        # Create contact network
        if network_type == 'random':
            p = avg_connections / (population_size - 1)
            G = nx.erdos_renyi_graph(population_size, p)
        elif network_type == 'scale_free':
            G = nx.barabasi_albert_graph(population_size, avg_connections // 2)
        else:  # small_world
            k = avg_connections if avg_connections % 2 == 0 else avg_connections - 1
            p_rewire = 0.1
            G = nx.watts_strogatz_graph(population_size, k, p_rewire)

        # Initialize states (0:S, 1:I, 2:R)
        states = np.zeros(population_size, dtype=int)
        patient_zero = np.random.randint(population_size)
        states[patient_zero] = 1

        daily_new_cases = [1]

        for _ in range(num_days - 1):
            new_cases = 0

            # For each infectious individual
            infectious = np.where(states == 1)[0]
            for inf in infectious:
                # Check recovery
                if np.random.random() < gamma:
                    states[inf] = 2
                    continue

                # Try to infect neighbors
                neighbors = list(G.neighbors(inf))
                susceptible_neighbors = [n for n in neighbors if states[n] == 0]

                for s in susceptible_neighbors:
                    if np.random.random() < beta:
                        states[s] = 1
                        new_cases += 1

            daily_new_cases.append(new_cases)

        # Calculate R0 (average degree * transmission probability / recovery rate)
        avg_degree = np.mean([G.degree(n) for n in G.nodes()])
        R0 = avg_degree * beta / gamma

        return daily_new_cases, R0

class MetapopulationSEIRSimulator:
    def __init__(self):
        pass

    def simulate(self, betas: List[float], sigmas: List[float], gammas: List[float],
                population_sizes: List[int], migration_rates: List[List[float]],
                num_days: int = 30) -> Tuple[List[List[int]], float]:
        """
        Simulate SEIR with multiple connected populations

        Args:
            betas: transmission rates for each population
            sigmas: exposure rates for each population
            gammas: recovery rates for each population
            population_sizes: size of each population
            migration_rates: matrix of migration rates between populations
            num_days: simulation duration
        """
        num_populations = len(population_sizes)

        # Initialize states for each population
        S = [n-1 if i==0 else n for i, n in enumerate(population_sizes)]
        E = [1 if i==0 else 0 for i in range(num_populations)]
        I = [0] * num_populations
        R = [0] * num_populations

        daily_new_cases = [[0] * num_populations]

        for day in range(num_days - 1):
            new_cases = [0] * num_populations

            # Within-population dynamics
            for pop in range(num_populations):
                # SEIR transitions
                new_exposed = np.random.binomial(S[pop],
                    1 - np.exp(-betas[pop] * I[pop] / population_sizes[pop]))
                new_infectious = np.random.binomial(E[pop],
                    1 - np.exp(-sigmas[pop]))
                recoveries = np.random.binomial(I[pop],
                    1 - np.exp(-gammas[pop]))

                S[pop] -= new_exposed
                E[pop] = E[pop] + new_exposed - new_infectious
                I[pop] = I[pop] + new_infectious - recoveries
                R[pop] += recoveries

                new_cases[pop] = new_infectious

            # Migration between populations
            for i in range(num_populations):
                for j in range(num_populations):
                    if i != j:
                        # Migrate susceptibles
                        migrants_s = np.random.binomial(S[i], migration_rates[i][j])
                        S[i] -= migrants_s
                        S[j] += migrants_s

                        # Migrate exposed
                        migrants_e = np.random.binomial(E[i], migration_rates[i][j])
                        E[i] -= migrants_e
                        E[j] += migrants_e

                        # Migrate infectious
                        migrants_i = np.random.binomial(I[i], migration_rates[i][j])
                        I[i] -= migrants_i
                        I[j] += migrants_i

                        # Migrate recovered
                        migrants_r = np.random.binomial(R[i], migration_rates[i][j])
                        R[i] -= migrants_r
                        R[j] += migrants_r

            daily_new_cases.append(new_cases)

        # Calculate overall R0 (simplified)
        R0 = np.mean([b/g for b, g in zip(betas, gammas)])

        return daily_new_cases, R0

class GravityModelSIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float,
                population_sizes: List[int],
                distances: List[List[float]],
                gravity_decay: float = 2.0,
                num_days: int = 30) -> Tuple[List[List[int]], float]:
        """
        Simulate SIR with gravity model for spatial spread

        Args:
            beta: base transmission rate
            gamma: recovery rate
            population_sizes: size of each population
            distances: matrix of distances between populations
            gravity_decay: power law decay with distance
            num_days: simulation duration
        """
        num_locations = len(population_sizes)

        # Initialize states for each location
        S = [n-1 if i==0 else n for i, n in enumerate(population_sizes)]
        I = [1 if i==0 else 0 for i in range(num_locations)]
        R = [0] * num_locations

        daily_new_cases = [[0] * num_locations]

        # Calculate gravity coupling terms
        coupling = np.zeros((num_locations, num_locations))
        for i in range(num_locations):
            for j in range(num_locations):
                if i != j:
                    coupling[i][j] = (population_sizes[i] * population_sizes[j] /
                                    (distances[i][j] ** gravity_decay))

        # Normalize coupling terms
        coupling = coupling / np.max(coupling)

        for day in range(num_days - 1):
            new_cases = [0] * num_locations

            for loc in range(num_locations):
                # Calculate force of infection from all locations
                lambda_i = beta * sum(coupling[loc][j] * I[j] / population_sizes[j]
                                    for j in range(num_locations))

                # New infections
                new_infections = np.random.binomial(S[loc],
                    1 - np.exp(-lambda_i))
                recoveries = np.random.binomial(I[loc],
                    1 - np.exp(-gamma))

                S[loc] -= new_infections
                I[loc] = I[loc] + new_infections - recoveries
                R[loc] += recoveries

                new_cases[loc] = new_infections

            daily_new_cases.append(new_cases)

        # Calculate R0 (simplified)
        R0 = beta/gamma

        return daily_new_cases, R0

class GillespieVectorBorneSimulator:
    def __init__(self):
        pass

    def simulate(self, beta_h: float, beta_v: float, gamma_h: float,
                mu_v: float, human_population: int = 10000,
                vector_population: int = 50000, num_days: int = 30) -> Tuple[List[int], float]:
        """
        Simulate vector-borne disease using Gillespie's SSA
        """
        # Initialize
        self.Nh = human_population
        self.Nv = vector_population

        Sh = self.Nh - 1
        Ih = 1
        Rh = 0

        Sv = self.Nv
        Iv = 0

        t = 0
        current_day = 0
        daily_new_cases = [1]
        new_cases_today = 0

        while t < num_days and (Ih > 0 or Iv > 0):
            # Calculate propensities
            human_infection_rate = beta_h * Sh * Iv / self.Nv
            vector_infection_rate = beta_v * Sv * Ih / self.Nh
            human_recovery_rate = gamma_h * Ih
            vector_death_rate = mu_v * Iv

            total_rate = (human_infection_rate + vector_infection_rate +
                         human_recovery_rate + vector_death_rate)

            if total_rate == 0:
                break

            dt = np.random.exponential(1/total_rate)
            t += dt

            while int(t) > current_day and current_day < num_days - 1:
                daily_new_cases.append(new_cases_today)
                new_cases_today = 0
                current_day += 1

            rand = np.random.random()
            if rand < human_infection_rate/total_rate:
                # Human infection
                Sh -= 1
                Ih += 1
                new_cases_today += 1
            elif rand < (human_infection_rate + vector_infection_rate)/total_rate:
                # Vector infection
                Sv -= 1
                Iv += 1
            elif rand < (human_infection_rate + vector_infection_rate +
                        human_recovery_rate)/total_rate:
                # Human recovery
                Ih -= 1
                Rh += 1
            else:
                # Vector death
                Iv -= 1
                Sv += 1

        while len(daily_new_cases) < num_days:
            daily_new_cases.append(0)

        R0 = np.sqrt((beta_h * beta_v)/(gamma_h * mu_v))
        return daily_new_cases, R0

class GillespieNetworkSIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, population_size: int = 10000,
                num_days: int = 30, network_type: str = 'random',
                avg_connections: int = 10) -> Tuple[List[int], float]:
        """
        Simulate SIR on a contact network using Gillespie's algorithm
        """
        import networkx as nx

        # Create network
        if network_type == 'random':
            p = avg_connections / (population_size - 1)
            G = nx.erdos_renyi_graph(population_size, p)
        elif network_type == 'scale_free':
            G = nx.barabasi_albert_graph(population_size, avg_connections // 2)
        else:  # small_world
            k = avg_connections if avg_connections % 2 == 0 else avg_connections - 1
            p_rewire = 0.1
            G = nx.watts_strogatz_graph(population_size, k, p_rewire)

        # Initialize states
        states = np.zeros(population_size, dtype=int)
        patient_zero = np.random.randint(population_size)
        states[patient_zero] = 1

        # Time tracking
        t = 0
        current_day = 0
        daily_new_cases = [1]
        new_cases_today = 0

        while t < num_days:
            # Get current infectious individuals and their susceptible neighbors
            infectious = np.where(states == 1)[0]
            if len(infectious) == 0:
                break

            # Calculate rates for all possible events
            infection_rates = []
            infection_pairs = []
            recovery_rates = []

            for inf in infectious:
                # Recovery events
                recovery_rates.append(gamma)

                # Infection events
                neighbors = list(G.neighbors(inf))
                susceptible_neighbors = [n for n in neighbors if states[n] == 0]

                for s in susceptible_neighbors:
                    infection_rates.append(beta)
                    infection_pairs.append((inf, s))

            # Total rate
            total_rate = sum(infection_rates) + sum(recovery_rates)

            if total_rate == 0:
                break

            # Time until next event
            dt = np.random.exponential(1/total_rate)
            t += dt

            # Record daily cases
            while int(t) > current_day and current_day < num_days - 1:
                daily_new_cases.append(new_cases_today)
                new_cases_today = 0
                current_day += 1

            # Choose and execute next event
            rand = np.random.random() * total_rate
            cumsum = 0

            # Check infection events
            for i, rate in enumerate(infection_rates):
                cumsum += rate
                if rand < cumsum:
                    # Execute infection
                    _, target = infection_pairs[i]
                    states[target] = 1
                    new_cases_today += 1
                    break

            # Check recovery events
            if rand >= cumsum:
                rand -= cumsum
                cumsum = 0
                for i, rate in enumerate(recovery_rates):
                    cumsum += rate
                    if rand < cumsum:
                        # Execute recovery
                        states[infectious[i]] = 2
                        break

        # Fill remaining days
        while len(daily_new_cases) < num_days:
            daily_new_cases.append(0)

        # Calculate R0 (average degree * transmission probability / recovery rate)
        avg_degree = np.mean([G.degree(n) for n in G.nodes()])
        R0 = avg_degree * beta / gamma

        return daily_new_cases, R0

class TauLeapingNetworkSIRSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, population_size: int = 10000,
                num_days: int = 30, network_type: str = 'random',
                avg_connections: int = 10, tau: float = 0.1) -> Tuple[List[int], float]:
        """
        Simulate SIR on a contact network using tau-leaping
        """
        import networkx as nx

        # Create network
        if network_type == 'random':
            p = avg_connections / (population_size - 1)
            G = nx.erdos_renyi_graph(population_size, p)
        elif network_type == 'scale_free':
            G = nx.barabasi_albert_graph(population_size, avg_connections // 2)
        else:  # small_world
            k = avg_connections if avg_connections % 2 == 0 else avg_connections - 1
            p_rewire = 0.1
            G = nx.watts_strogatz_graph(population_size, k, p_rewire)

        # Initialize states
        states = np.zeros(population_size, dtype=int)
        patient_zero = np.random.randint(population_size)
        states[patient_zero] = 1

        steps_per_day = int(1/tau)
        daily_new_cases = [1]

        for day in range(num_days - 1):
            new_cases_today = 0

            for _ in range(steps_per_day):
                # Get current infectious individuals
                infectious = np.where(states == 1)[0]
                if len(infectious) == 0:
                    break

                # Store all transitions to execute simultaneously
                new_infections = set()
                new_recoveries = set()

                # Process infectious individuals
                for inf in infectious:
                    # Check recovery
                    if np.random.poisson(gamma * tau) > 0:
                        new_recoveries.add(inf)
                        continue

                    # Try to infect neighbors
                    neighbors = list(G.neighbors(inf))
                    susceptible_neighbors = [n for n in neighbors if states[n] == 0]

                    for s in susceptible_neighbors:
                        if np.random.poisson(beta * tau) > 0:
                            new_infections.add(s)

                # Execute transitions
                for node in new_recoveries:
                    states[node] = 2
                for node in new_infections:
                    if states[node] == 0:  # Check still susceptible
                        states[node] = 1
                        new_cases_today += 1

            daily_new_cases.append(new_cases_today)

        # Calculate R0
        avg_degree = np.mean([G.degree(n) for n in G.nodes()])
        R0 = avg_degree * beta / gamma

        return daily_new_cases, R0

class SISSimulator:
    def __init__(self):
        pass

    def simulate(self, beta: float, gamma: float, population_size: int = 10000,
                num_days: int = 30) -> Tuple[List[int], float]:
        """
        Simulate SIS epidemic model

        Args:
            beta: transmission rate
            gamma: recovery rate
            population_size: total population
            num_days: simulation duration

        Returns:
            daily_new_cases: list of daily new cases
            R0: basic reproduction number
        """
        # Initialize states (0:S, 1:I)
        S = population_size - 1
        I = 1

        daily_new_cases = [1]  # Start with patient zero

        for _ in range(num_days - 1):
            # Calculate transitions
            # New infections
            new_infections = np.random.binomial(S,
                1 - np.exp(-beta * I / population_size))

            # Recoveries (who become susceptible again)
            recoveries = np.random.binomial(I,
                1 - np.exp(-gamma))

            # Update states
            S = S - new_infections + recoveries
            I = I + new_infections - recoveries

            daily_new_cases.append(new_infections)

        # Calculate R0 (transmission rate / recovery rate)
        R0 = beta / gamma

        return daily_new_cases, R0

class AgeStructuredSimulator:
    def __init__(self):
        pass

    def simulate(self, contact_matrix, betas, gammas, population_sizes=[5000, 3000, 2000], num_days=30):
        self.N = population_sizes
        n_groups = len(self.N)
        S = [n-1 if i==0 else n for i, n in enumerate(self.N)]
        I = [1 if i==0 else 0 for i in range(n_groups)]
        R = [0] * n_groups

        daily_new_cases = [[1 if i==0 else 0 for i in range(n_groups)]]

        for _ in range(num_days - 1):
            new_infections = []
            recoveries = []

            for i in range(n_groups):
                # Calculate force of infection with safeguard against division by zero
                lambda_i = 0
                for j in range(n_groups):
                    if self.N[j] > 0 and I[j] >= 0:  # Add checks
                        lambda_i += contact_matrix[i][j] * betas[j] * I[j] / self.N[j]

                # Ensure probability is valid
                p = 1 - np.exp(-max(0, min(lambda_i, 100)))  # Clip lambda_i to avoid overflow
                p = max(0, min(1, p))  # Ensure probability is between 0 and 1

                new_inf = np.random.binomial(max(0, S[i]), p)  # Ensure non-negative population
                rec = np.random.binomial(max(0, I[i]), 1 - np.exp(-gammas[i]))

                new_infections.append(new_inf)
                recoveries.append(rec)

            # Update compartments with bounds checking
            for i in range(n_groups):
                S[i] = max(0, S[i] - new_infections[i])
                I[i] = max(0, I[i] + new_infections[i] - recoveries[i])
                R[i] = max(0, R[i] + recoveries[i])

            daily_new_cases.append(new_infections)

        NGM = np.array([[contact_matrix[i][j] * betas[j] / gammas[i]
                        for j in range(n_groups)] for i in range(n_groups)])
        R0 = max(abs(np.linalg.eigvals(NGM)))

        return daily_new_cases, R0

class DiseaseSimulationDataset(Dataset):
    def __init__(self, num_samples=3_000_000, seed=42):
        self.simulators = {
            'sir': SIRSimulator(),
            'sis': SISSimulator(),
            'seir': SEIRSimulator(),
            'sirs': SIRSSimulator(),
            'age': AgeStructuredSimulator(),
        }
        self.num_samples = num_samples
        self.seed = seed
        random.seed(seed)  # Set seed for reproducibility

        # Initialize storage for simulator types and parameters
        self.simulator_types = [None] * num_samples
        self.params_used = [None] * num_samples

        self.data = []
        self.r0s = []
        self.days_of_year = []

    def _add_reporting_effects(self, true_cases):
        """Add realistic but variable reporting effects to case counts"""
        cases = true_cases.copy()

        # Randomly decide which effects to apply
        apply_delays = random.random() < 0.7  # 70% chance of having delays
        apply_weekend = random.random() < 0.6  # 60% chance of weekend effect
        apply_reporting_rate = random.random() < 0.6  # 80% chance of incomplete reporting
        apply_batch = random.random() < 0.3  # 30% chance of batch reporting

        if apply_reporting_rate:
            # Time-varying reporting rate
            base_reporting_rate = random.uniform(0.5, 0.9)
            reporting_noise = np.random.normal(0, 0.1, len(cases))
            reporting_rate = np.clip(base_reporting_rate + reporting_noise, 0.3, 0.95)
            cases = np.random.binomial(cases.astype(int), reporting_rate)

        if apply_delays:
            # Reporting delays with variable delay distribution
            max_delay = random.randint(2, 7)
            delay_dist = np.random.dirichlet(np.ones(max_delay))

            delayed_cases = np.zeros_like(cases)
            for t in range(len(cases)):
                for d, p in enumerate(delay_dist):
                    if t + d < len(delayed_cases):
                        delayed_cases[t + d] += cases[t] * p
            cases = delayed_cases

        if apply_weekend:
            # Weekend effect with variable strength
            weekend_effect = np.ones_like(cases)
            sunday_effect = random.uniform(0.4, 0.8)
            saturday_effect = random.uniform(0.6, 0.9)

            offset = random.randint(0, 6)
            weekend_effect[(6 + offset)::7] *= sunday_effect
            weekend_effect[(5 + offset)::7] *= saturday_effect

            cases = cases * weekend_effect

        if apply_batch:
            # Simulate batch reporting effects
            num_batches = random.randint(1, 3)  # 1-3 batch reporting events
            for _ in range(num_batches):
                # Choose a random day for the batch report
                batch_day = random.randint(0, len(cases)-1)

                # Accumulate cases from previous days
                accumulation_days = random.randint(2, 7)  # 2-7 days of accumulated cases
                start_day = max(0, batch_day - accumulation_days)

                # Store the cases that will be moved
                accumulated_cases = cases[start_day:batch_day].copy()

                # Remove these cases from their original days
                cases[start_day:batch_day] *= random.uniform(0.1, 0.3)  # Leave some cases in original days

                # Add accumulated cases to the batch day
                cases[batch_day] += accumulated_cases.sum()

                # Add some extra noise to the batch
                cases[batch_day] *= random.uniform(0.9, 1.1)

        return np.round(cases).astype(int)

    def _add_stochastic_events(self, cases, num_days):
        """Add imported cases and super-spreader events"""
        modified_cases = cases.copy()

        # Random imported cases
        import_rate = random.uniform(0.01, 0.1)  # Base rate of imports
        imported = np.random.poisson(import_rate * modified_cases.mean(), num_days)
        modified_cases += imported

        # Super-spreader events
        num_events = random.randint(0, 3)  # 0-3 super spreader events
        for _ in range(num_events):
            # Events more likely during growth phase
            day = random.randint(0, num_days-1)
            size = int(np.random.gamma(5, 4))  # Gamma distribution for event size
            modified_cases[day] += size

        return modified_cases

    def _add_noise(self, cases):
        """Add various types of noise to case counts"""
        # Overdispersion in case reporting (negative binomial noise)
        r = 10  # Dispersion parameter
        p = r / (r + cases)
        cases = np.random.negative_binomial(r, p)

        # Random multiplicative noise
        noise_factor = np.random.lognormal(mean=0, sigma=0.1, size=len(cases))
        cases = cases * noise_factor

        # Round to integers
        return np.round(cases).astype(int)

    def _augment_epidemic_speed(self, cases, r0, num_augmentations=2):
        """Create versions of the epidemic with different speeds"""
        from scipy import signal

        augmented_cases = [cases]  # Keep original
        augmented_r0s = [r0]

        for _ in range(num_augmentations):
            # Random speed factor: 0.5 = twice as slow, 2.0 = twice as fast
            speed_factor = random.uniform(0.4, 1.1)

            # Calculate new length for resampling
            orig_len = len(cases)
            new_len = int(orig_len * speed_factor)

            # Resample the epidemic curve
            stretched = scipy.signal.resample(cases, new_len)

            # Trim or pad to original length
            if len(stretched) > orig_len:
                # Take center portion if stretched
                start_idx = (len(stretched) - orig_len) // 2
                stretched = stretched[start_idx:start_idx + orig_len]
            else:
                # Pad with zeros if compressed
                pad_left = (orig_len - len(stretched)) // 2
                pad_right = orig_len - len(stretched) - pad_left
                stretched = np.pad(stretched, (pad_left, pad_right))

            augmented_cases.append(stretched)
            augmented_r0s.append(r0)  # R0 remains same, only speed changes

        return augmented_cases, augmented_r0s

    def _get_random_params(self, simulator_type):
        """Get random parameters based on simulator type"""
        population_size = int(np.exp(np.random.uniform(np.log(1000), np.log(1000000))))

        # Define disease characteristics
        disease_characteristics = {
            'is_waterborne': False,
            'is_vectorborne': False,
            'has_latent_period': False,
            'has_waning_immunity': False
        }

        params = {}

        if simulator_type == 'sis':  # Add SIS parameter generation
            params = {
                'beta': random.uniform(0.1, 0.45),
                'gamma': random.uniform(0.05, 0.5),
                'population_size': population_size
            }
            disease_characteristics['has_waning_immunity'] = True

        elif simulator_type == 'siwr':  # Add SIWR parameter generation
            params = {
                'beta': random.uniform(0.1, 0.5),
                'gamma': random.uniform(0.05, 0.25),
                'omega': random.uniform(0.01, 0.1),
                'delta': random.uniform(0.01, 0.1),
                'water_contamination_rate': random.uniform(0.001, 0.05),
                'population_size': population_size
            }
            disease_characteristics['is_waterborne'] = True
            disease_characteristics['has_waning_immunity'] = True

        elif simulator_type == 'sir':
            params = {
                'beta': random.uniform(0.1, 0.45),
                'gamma': random.uniform(0.05, 0.5),
                'population_size': population_size
            }

        elif simulator_type == 'seir':
            params = {
                'beta': random.uniform(0.1, 0.45),
                'gamma': random.uniform(0.05, 0.5),
                'sigma': random.uniform(0.1, 0.5),
                'population_size': population_size
            }
            disease_characteristics['has_latent_period'] = True

        elif simulator_type == 'sirs':
            params = {
                'beta': random.uniform(0.1, 0.45),
                'gamma': random.uniform(0.05, 0.5),
                'omega': random.uniform(0.01, 0.1),
                'population_size': population_size
            }
            disease_characteristics['has_waning_immunity'] = True

        elif simulator_type == 'vector':
          human_population = population_size
          vector_population = int(population_size * random.uniform(2, 10))  # 2-10x more vectors than humans
          params = {
              'beta_h': random.uniform(0.1, 0.45),
              'beta_v': random.uniform(0.1, 0.45),
              'gamma_h': random.uniform(0.05, 0.5),
              'mu_v': random.uniform(0.1, 0.3),
              'human_population': human_population,
              'vector_population': vector_population
          }
          disease_characteristics['has_waning_immunity'] = True

        elif simulator_type == 'age':
            # Create age distribution
            young_frac = random.uniform(0.2, 0.4)
            adult_frac = random.uniform(0.4, 0.6)
            elderly_frac = 1 - young_frac - adult_frac

            population_sizes = [
                int(population_size * young_frac),
                int(population_size * adult_frac),
                int(population_size * elderly_frac)
            ]
            contact_matrix = np.random.uniform(0.1, 1.0, (3, 3))
            contact_matrix = (contact_matrix + contact_matrix.T) / 2

            params = {
                'contact_matrix': contact_matrix,
                'betas': np.random.uniform(0.1, 0.5, 3),
                'gammas': np.random.uniform(0.05, 0.5, 3),
                'population_sizes': population_sizes
            }

        # Add disease characteristics to params
        params['disease_characteristics'] = disease_characteristics
        return params, population_size

    def _generate_data(self, num_samples):
        print("Generating disease simulations...")
        generated = 0
        attempts = 0
        max_attempts = num_samples * 5  # Allow up to 5x attempts to get enough valid samples

        while generated < num_samples and attempts < max_attempts:
            attempts += 1

            simulator_type = random.choice(list(self.simulators.keys()))
            simulator = self.simulators[simulator_type]

            params = self._get_random_params(simulator_type)
            seq_length = random.randint(14, 100)

            # Generate true cases trajectory
            true_cases, r0 = simulator.simulate(**params, num_days=seq_length)

            # For age-structured model, sum across age groups
            if simulator_type == 'age':
                true_cases = [sum(day) for day in true_cases]

            # Add stochastic events
            cases = self._add_stochastic_events(true_cases, seq_length)

            # Add noise
            cases = self._add_noise(cases)

            # Add reporting effects
            reported_cases = self._add_reporting_effects(true_cases)

            # Check if this is a valid outbreak
            total_cases = sum(cases)
            if total_cases >= 30:
                # Create speed variations
                aug_cases, aug_r0s = self._augment_epidemic_speed(cases, r0)

                # Add all versions to dataset
                for case, r0_val in zip(aug_cases, aug_r0s):
                    self.data.append(torch.tensor(case, dtype=torch.float32))
                    self.r0s.append(r0_val)
                    self.days_of_year.append(torch.tensor([0] * seq_length, dtype=torch.long))
                    generated += 1

                    if generated % 10000 == 0:
                        print(f"Generated {generated}/{num_samples} valid outbreaks "
                            f"(Attempt rate: {generated/attempts:.1%})")

            if generated >= num_samples:
                break

        if generated < num_samples:
            print(f"Warning: Only generated {generated}/{num_samples} valid outbreaks "
                f"after {attempts} attempts")

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        max_attempts = 100
        attempt = 0
        max_seq_length = 100

        while attempt < max_attempts:
            random.seed(self.seed + idx + attempt)

            simulator_type = random.choice(list(self.simulators.keys()))
            simulator = self.simulators[simulator_type]

            # Get parameters and calculate R0 before simulation
            params, population_size = self._get_random_params(simulator_type)

            # Store disease characteristics separately and remove from params
            disease_characteristics = params.pop('disease_characteristics')

            # Calculate R0 based on simulator type
            if simulator_type == 'sis':
                calculated_r0 = params['beta'] / params['gamma']
            elif simulator_type == 'sir' or simulator_type == 'sirs':
                calculated_r0 = params['beta'] / params['gamma']
            elif simulator_type == 'seir':
                calculated_r0 = params['beta'] / params['gamma']
            elif simulator_type == 'vector':
                calculated_r0 = np.sqrt((params['beta_h'] * params['beta_v'])/(params['gamma_h'] * params['mu_v']))
            elif simulator_type == 'age':
                NGM = np.array([[params['contact_matrix'][i][j] * params['betas'][j] / params['gammas'][i]
                                for j in range(3)] for i in range(3)])
                calculated_r0 = max(abs(np.linalg.eigvals(NGM)))

            # Skip if R0 < 0.9
            if calculated_r0 < 0.9:
                attempt += 1
                continue

            seq_length = random.randint(14, max_seq_length)

            # Generate trajectory
            new_cases, r0 = simulator.simulate(**params, num_days=seq_length)

            # For age-structured model, sum across age groups
            if simulator_type == 'age':
                new_cases = [sum(day) for day in new_cases]

            # Check if this is a valid outbreak
            if sum(new_cases) >= 15:
                # Pad sequence to max_seq_length
                data = torch.tensor(new_cases, dtype=torch.float32)
                if len(data) < max_seq_length:
                    data = F.pad(data, (0, max_seq_length - len(data)))
                days = torch.zeros(max_seq_length, dtype=torch.long)
                population_tensor = torch.tensor(population_size, dtype=torch.float32)
                r0 = torch.tensor(r0, dtype=torch.float32)

                # Convert disease characteristics to tensor
                disease_chars = torch.tensor([
                    float(disease_characteristics['is_waterborne']),
                    float(disease_characteristics['is_vectorborne']),
                    float(disease_characteristics['has_latent_period']),
                    float(disease_characteristics['has_waning_immunity'])
                ], dtype=torch.float32)

                # Store what was used to generate this sample
                self.simulator_types[idx] = simulator_type
                self.params_used[idx] = {**params, 'disease_characteristics': disease_characteristics}
                return data, days, r0, simulator_type, self.params_used[idx], disease_chars, population_tensor

            attempt += 1

        # If we get here, try one last time with parameters that will definitely give R0 > 0.9
        if simulator_type == 'sis':
            params['beta'] = random.uniform(0.5, 1.0)
            params['gamma'] = min(0.5, params['beta']/0.9)
        elif simulator_type == 'siwr':
            params['beta'] = random.uniform(0.5, 1.0)
            params['gamma'] = min(0.5, params['beta']/0.9)
        elif simulator_type == 'sir' or simulator_type == 'sirs':
            params['beta'] = random.uniform(0.5, 1.0)
            params['gamma'] = min(0.5, params['beta']/0.9)  # Ensure R0 > 0.9
        elif simulator_type == 'seir':
            params['beta'] = random.uniform(0.5, 1.0)
            params['gamma'] = min(0.5, (params['beta'] * params['sigma'])/(0.9 * (params['sigma'] + 0.9)))
        elif simulator_type == 'vector':
            params['beta_h'] = random.uniform(0.5, 1.0)
            params['beta_v'] = random.uniform(0.5, 1.0)

        new_cases, r0 = simulator.simulate(**params, num_days=seq_length)
        if simulator_type == 'age':
            new_cases = [sum(day) for day in new_cases]

        data = torch.tensor(new_cases, dtype=torch.float32)
        if len(data) < max_seq_length:
            data = F.pad(data, (0, max_seq_length - len(data)))
        days = torch.zeros(max_seq_length, dtype=torch.long)
        r0 = torch.tensor(r0, dtype=torch.float32)
        population_tensor = torch.tensor(population_size, dtype=torch.float32)

        # Convert disease characteristics to tensor
        disease_chars = torch.tensor([
            float(disease_characteristics['is_waterborne']),
            float(disease_characteristics['is_vectorborne']),
            float(disease_characteristics['has_latent_period']),
            float(disease_characteristics['has_waning_immunity'])
        ], dtype=torch.float32)

        # Store what was used to generate this sample
        self.simulator_types[idx] = simulator_type
        self.params_used[idx] = {**params, 'disease_characteristics': disease_characteristics}
        return data, days, r0, simulator_type, self.params_used[idx], disease_chars, population_tensor


# Test the dataset
def test_dataset():
    dataset = DiseaseSimulationDataset(num_samples=100)

    # Test a few random samples
    for i in range(5):
        data, days, r0, simulator_type, params, disease_chars, population = dataset[i]
        print(f"\nSample {i}:")
        print(f"Trajectory length: {len(data)}")
        print(f"First 5 daily cases: {data[:5].numpy()}")
        print(f"R0: {r0}")
        print(f"Total cases: {sum(data.numpy())}")
        print(f"Simulator type: {simulator_type}")
        print(f"Disease characteristics:")
        print(f"  Waterborne: {bool(disease_chars[0])}")
        print(f"  Vectorborne: {bool(disease_chars[1])}")
        print(f"  Has latent period: {bool(disease_chars[2])}")
        print(f"  Has waning immunity: {bool(disease_chars[3])}")

        # Plot the trajectory
        plt.figure(figsize=(10, 4))
        plt.plot(data.numpy())
        plt.title(f"Sample {i} (R0={r0:.2f}, {simulator_type})")
        plt.xlabel("Day")
        plt.ylabel("New Cases")
        plt.show()

test_dataset()

class TimeAwareAttention(nn.Module):
   def __init__(self, d_model, n_heads, max_len=365):
       super().__init__()
       self.d_model = d_model
       self.n_heads = n_heads

       # Seasonal patterns embedding
       self.day_embedding = nn.Embedding(max_len, d_model)

       # Learnable timescale embeddings
       self.timescale_embeddings = nn.Parameter(torch.randn(3, d_model))  # daily, weekly, monthly

       # Multi-head attention
       self.mha = nn.MultiheadAttention(d_model, n_heads, batch_first=True)

   def forward(self, x, days_of_year=None):
       B, L, D = x.shape

       # Add seasonal context if provided
       if days_of_year is not None:
           seasonal_embedding = self.day_embedding(days_of_year)
           x = x + seasonal_embedding

       # Add multi-scale temporal attention
       for i, timescale in enumerate([1, 7, 30]):  # daily, weekly, monthly
           indices = torch.arange(L, device=x.device)
           time_encoding = torch.sin(indices.float() * (2 * np.pi / timescale))
           x = x + self.timescale_embeddings[i] * time_encoding.unsqueeze(-1)

       return self.mha(x, x, x)[0]

class SEBlock(nn.Module):
    def __init__(self, channel, reduction=16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool1d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.GELU(),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _ = x.shape
        y = self.squeeze(x).view(b, c)
        y = self.excitation(y).view(b, c, 1)
        return x * y

class AttentivePooling(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.attention = nn.Linear(d_model, 1)

    def forward(self, x):
        # x shape: [batch, seq_len, d_model]
        weights = F.softmax(self.attention(x), dim=1)  # [batch, seq_len, 1]
        return torch.sum(weights * x, dim=1)

class MultiScaleTemporalBlock(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.scales = [1, 7, 14, 28]  # Daily, weekly, bi-weekly, monthly

        # Multi-scale pooling and processing
        self.pools = nn.ModuleList([
            nn.Sequential(
                # Ensure same output size by adjusting padding
                nn.AvgPool1d(
                    scale,
                    stride=1,
                    padding=scale//2 if scale % 2 != 0 else (scale//2 - 1)
                ),
                nn.Conv1d(d_model, d_model, 3, padding='same'),
                nn.GELU(),
                nn.BatchNorm1d(d_model)
                # SEBlock(d_model)
            ) for scale in self.scales
        ])

        # Projection after concatenation
        self.projection = nn.Conv1d(len(self.scales) * d_model, d_model, 1)

    def forward(self, x):
        # x shape: [batch, d_model, seq_length]
        seq_len = x.size(-1)
        multi_scale_features = []

        for pool in self.pools:
            features = pool(x)
            # Ensure all features have the same sequence length
            if features.size(-1) != seq_len:
                features = F.interpolate(
                    features,
                    size=seq_len,
                    mode='linear',
                    align_corners=False
                )
            multi_scale_features.append(features)

        # Concatenate along channel dimension
        x_multi = torch.cat(multi_scale_features, dim=1)
        # Project back to d_model dimensions
        return self.projection(x_multi)

class InputNormalizer(nn.Module):
    def __init__(self, seq_length, normalization_types=['scale', 'moving_average', 'log']):
        super().__init__()
        self.seq_length = seq_length
        self.normalization_types = normalization_types

        # Learnable parameters for scaling
        self.scale_factor = nn.Parameter(torch.ones(1))
        self.shift_factor = nn.Parameter(torch.zeros(1))

        # Moving average window sizes
        self.window_sizes = [3, 7, 14]  # Different smoothing windows

    def forward(self, x):
        # Input shape: [batch_size, seq_length]
        normalized_features = []

        if 'scale' in self.normalization_types:
            # Min-max normalization with learnable parameters
            x_min = x.min(dim=1, keepdim=True)[0]
            x_max = x.max(dim=1, keepdim=True)[0]
            x_scaled = (x - x_min) / (x_max - x_min + 1e-6)
            x_scaled = x_scaled * self.scale_factor + self.shift_factor
            normalized_features.append(x_scaled)

        if 'moving_average' in self.normalization_types:
            # Multi-window moving averages
            for window in self.window_sizes:
                # Pad for causal convolution
                x_pad = F.pad(x.unsqueeze(1), (window-1, 0))
                ma = F.avg_pool1d(x_pad, kernel_size=window, stride=1)
                # Normalize by moving average
                x_ma = x.unsqueeze(1) / (ma + 1e-6)
                normalized_features.append(x_ma.squeeze(1))

        if 'log' in self.normalization_types:
            # Log transformation with offset to handle zeros
            x_log = torch.log(x + 1)
            normalized_features.append(x_log)

        if 'growth_rate' in self.normalization_types:
            # Calculate daily growth rates
            growth_rates = torch.log(x[:, 1:] + 1) - torch.log(x[:, :-1] + 1)
            growth_rates = F.pad(growth_rates, (1, 0))  # Pad first day
            normalized_features.append(growth_rates)

        if 'population_scaled' in self.normalization_types:
            # Scale by population size (if available)
            population_sizes = self.get_population_sizes()  # Would need to be implemented
            cases_per_capita = x / (population_sizes.unsqueeze(1) + 1e-6)
            normalized_features.append(cases_per_capita)

        # Combine all normalizations
        x_normalized = torch.stack(normalized_features, dim=1)
        return x_normalized


class InfectiousDiseaseR0Extractor(nn.Module):
    def __init__(
        self,
        seq_length: int,
        d_model: int = 256,
        nhead: int = 8,
        num_encoder_layers: int = 6,
        dim_feedforward: int = 1024,
        dropout: float = 0.1,
        num_cnn_layers: int = 3,
        normalization_types: list = ['scale', 'moving_average', 'log']
    ):
        super().__init__()

        # Add input normalization
        self.input_normalizer = InputNormalizer(
            seq_length=seq_length,
            normalization_types=normalization_types
        )

        # Initial projection layer for normalized features
        num_norm_features = len(normalization_types) + 2
        self.initial_projection = nn.Linear(num_norm_features, d_model)

        # Add attentive pooling
        self.attentive_pooling = AttentivePooling(d_model)

        # 1. Multi-scale temporal feature extraction
        self.cnn_layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(d_model, d_model, kernel_size=3+2*i, padding='same'),
                nn.GELU(),
                nn.BatchNorm1d(d_model)
                # SEBlock(d_model)
            ) for i in range(num_cnn_layers)
        ])

        # CNN feature projection
        self.cnn_projection = nn.Linear(num_cnn_layers * d_model, d_model)

        # Add multi-scale temporal block after CNN layers
        self.multi_scale = MultiScaleTemporalBlock(d_model)

        # 2. Position encodings for different timescales
        self.pos_encoding = nn.Parameter(torch.randn(1, seq_length, d_model))

        # 3. Transformer encoder with relative position encoding
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_encoder_layers
        )

        # 4. Growth rate estimation head
        self.growth_head = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.GELU(),
            nn.Linear(d_model//2, 1)
        )

        # 5. Serial interval estimation head
        self.serial_head = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.GELU(),
            nn.Linear(d_model//2, 1),
            nn.Softplus()  # Ensure positive
        )

        # 6. Final R0 estimation with uncertainty
        self.final_layer = nn.Sequential(
            nn.Linear(d_model + 2, d_model//2),  # Takes transformed features + growth + serial
            nn.GELU(),
            nn.Linear(d_model//2, 3)  # Output: [R0_mean, R0_lower, R0_upper]
        )

        self.disease_char_projection = nn.Sequential(
            nn.Linear(4, d_model // 2),
            nn.GELU(),
            nn.Linear(d_model // 2, d_model)
        )

        # Modify final layer to include disease characteristics
        self.final_layer = nn.Sequential(
            nn.Linear(d_model * 2 + 2, d_model),  # Doubled d_model to account for disease chars
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Linear(d_model // 2, 3)  # Output: [R0_mean, R0_lower, R0_upper]
        )

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.to(self.device)

    def forward(self, x, days_of_year=None, disease_chars=None):

        # Process disease characteristics
        if disease_chars is not None:
            disease_features = self.disease_char_projection(disease_chars)  # [batch, d_model]
        else:
            disease_features = torch.zeros(x.size(0), self.d_model, device=x.device)


        # 1. Normalize input
        x_normalized = self.input_normalizer(x)  # [batch, num_features, seq_length]

        # 2. Project normalized features
        # Transpose to get features dimension last [batch, seq_length, num_features]
        x = x_normalized.transpose(1, 2)

        # Project to d_model dimension [batch, seq_length, d_model]
        x = self.initial_projection(x)

        # 3. Prepare for CNN [batch, d_model, seq_length]
        x = x.transpose(1, 2)

        # 3. CNN feature extraction
        cnn_features = []
        for cnn in self.cnn_layers:
            residual = x
            features = cnn(x)
            features = features + residual  # Add residual connection
            cnn_features.append(features)
            x = features

        # Combine CNN features
        x = torch.cat(cnn_features, dim=1)
        x = x.transpose(1, 2)
        x = self.cnn_projection(x)
        x = x.transpose(1, 2)  # [batch, d_model, seq_length]

        # Add multi-scale temporal processing
        x = self.multi_scale(x)  # [batch, d_model, seq_length]

        # Prepare for transformer
        x = x.transpose(1, 2)  # [batch, seq_length, d_model]
        x = x + self.pos_encoding

        # 6. Transform through encoder
        transformed = self.transformer(x)

        # 7. Global feature pooling
        global_features = self.attentive_pooling(transformed)  # [batch, d_model]

        # 8. Calculate outputs
        growth_rate = self.growth_head(global_features)
        serial_interval = self.serial_head(global_features)

        combined = torch.cat([
            global_features,
            disease_features,
            growth_rate,
            serial_interval
        ], dim=-1)

        r0_estimates = self.final_layer(combined)
        r0_lower, r0_mean, r0_upper = r0_estimates[:, 0], r0_estimates[:, 1], r0_estimates[:, 2]

        return r0_lower, r0_mean, r0_upper

    def forward_with_explanation(self, x, days_of_year=None):
        """Forward pass that returns intermediate calculations for interpretation"""
        x = x.unsqueeze(1)

        # Store CNN features
        cnn_features = []
        feature_maps = []
        for i, cnn in enumerate(self.cnn_layers):
            features = cnn(x)
            feature_maps.append(features.detach().cpu())  # Save feature maps
            cnn_features.append(features)
            x = features

        x = torch.cat(cnn_features, dim=1)
        x = x.transpose(1, 2)
        x = self.feature_projection(x)

        x = x + self.pos_encoding
        transformed = self.transformer(x)
        global_features = torch.mean(transformed, dim=1)

        # Get intermediate predictions
        growth_rate = self.growth_head(global_features)
        serial_interval = self.serial_head(global_features)

        combined = torch.cat([
            global_features,
            growth_rate,
            serial_interval
        ], dim=-1)

        r0_estimates = self.final_layer(combined)
        r0_lower, r0_mean, r0_upper = r0_estimates[:, 0], r0_estimates[:, 1], r0_estimates[:, 2]

        explanation = {
            'feature_maps': feature_maps,  # CNN feature maps at each scale
            'growth_rate': growth_rate.detach().cpu(),  # Estimated epidemic growth rate
            'serial_interval': serial_interval.detach().cpu(),  # Estimated serial interval
            'global_features': global_features.detach().cpu()  # Global sequence features
        }

        return r0_lower, r0_mean, r0_upper, explanation

    def interpret_prediction(self, x, days_of_year=None):
        """Provide a human-readable interpretation of the model's prediction"""
        r0_lower, r0_mean, r0_upper, explanation = self.forward_with_explanation(x, days_of_year)

        # Get values from tensors
        growth_rate = explanation['growth_rate'].numpy()
        serial_interval = explanation['serial_interval'].numpy()

        interpretation = f"""
        The model analyzed the epidemic curve and found:

        1. Growth Rate: {growth_rate[0]:.3f}
          - This indicates how quickly cases are increasing
          - A positive value suggests exponential growth
          - A negative value suggests decay

        2. Serial Interval: {serial_interval[0]:.3f} days
          - This estimates the average time between successive cases
          - Shorter intervals suggest faster transmission chains
          - Longer intervals suggest slower transmission

        3. R0 Estimate: {r0_mean[0]:.2f} (95% CI: {r0_lower[0]:.2f} - {r0_upper[0]:.2f})
          - Combines growth rate and serial interval
          - Values > 1 indicate epidemic growth
          - Wider confidence intervals suggest more uncertainty

        The model used {len(explanation['feature_maps'])} different temporal scales
        to analyze the pattern of cases over time.
        """

        return interpretation

class OutbreakMemory(nn.Module):  # Add nn.Module inheritance
    def __init__(self, max_size=65536):
        super().__init__()
        self.max_size = max_size

        # Memory buffers should be registered but not initialized yet
        self.register_buffer('cases', torch.zeros(0, 14))
        self.register_buffer('populations', torch.zeros(0))
        self.register_buffer('disease_chars', torch.zeros(0, 4))
        self.register_buffer('r0_values', torch.zeros(0))
        self.current_size = 0

        # Learnable parameters should be created on the same device as model
        self.case_sim_weight = nn.Parameter(torch.tensor(0.6, requires_grad=True, dtype=torch.float32))
        self.pop_sim_weight = nn.Parameter(torch.tensor(0.4, requires_grad=True, dtype=torch.float32))

    def add_outbreaks_batch(self, cases, populations, disease_chars, r0s):
        """Add a batch of outbreaks to memory efficiently"""
        batch_size = cases.size(0)
        available_space = self.max_size - self.current_size
        device = self.case_sim_weight.device  # Use this as reference device

        if available_space <= 0:
            # If memory is full, remove oldest entries
            remove_count = min(batch_size, self.max_size)
            self.cases = self.cases[remove_count:]
            self.populations = self.populations[remove_count:]
            self.disease_chars = self.disease_chars[remove_count:]
            self.r0_values = self.r0_values[remove_count:]
            self.current_size = len(self.cases)

        # Add new cases (keep everything on the same device)
        add_count = min(batch_size, available_space)
        new_cases = cases[:add_count, :14].to(device)
        new_pops = populations[:add_count].to(device)
        new_chars = disease_chars[:add_count].to(device)
        new_r0s = r0s[:add_count].to(device)

        self.cases = torch.cat([self.cases, new_cases])
        self.populations = torch.cat([self.populations, new_pops])
        self.disease_chars = torch.cat([self.disease_chars, new_chars])
        self.r0_values = torch.cat([self.r0_values, new_r0s])
        self.current_size = len(self.cases)

    def get_similar_outbreaks_batch(self, query_cases, query_populations, query_chars, k=3):
        """Get similar outbreaks for entire batch at once"""
        if self.current_size == 0:
            batch_size = query_cases.size(0)
            device = query_cases.device
            return (
                torch.zeros(batch_size, k, 14, device=device),
                torch.zeros(batch_size, k, device=device),
                torch.zeros(batch_size, k, device=device)
            )

        # Everything should already be on the correct device
        batch_size = query_cases.size(0)

        # Ensure query cases are the right shape [batch, 14]
        if len(query_cases.shape) == 3:
            query_cases = query_cases[:, :14, 0]
        elif len(query_cases.shape) == 2:
            query_cases = query_cases[:, :14]

        query_cases = query_cases.float().contiguous()

        # Calculate characteristic matches
        char_matches = torch.all(
            torch.abs(
                query_chars.unsqueeze(1) - self.disease_chars.unsqueeze(0)
            ) < 1e-6,
            dim=2
        )

        # Calculate population similarities using log difference
        log_query_pop = torch.log(query_populations.unsqueeze(1) + 1)
        log_memory_pop = torch.log(self.populations.unsqueeze(0) + 1)
        pop_diff = torch.abs(log_query_pop - log_memory_pop)

        # Normalize log differences to [0,1] range
        pop_similarities = torch.exp(-pop_diff)

        # Case similarities
        case_similarities = F.cosine_similarity(
            query_cases.unsqueeze(1),
            self.cases.unsqueeze(0),
            dim=2
        )

        # Use softmax to ensure weights sum to 1
        weights = F.softmax(torch.stack([self.case_sim_weight, self.pop_sim_weight]), dim=0)

        # Combine similarities with learned weights
        total_similarity = torch.where(
            char_matches,
            weights[0] * case_similarities + weights[1] * pop_similarities,
            torch.full_like(case_similarities, float('-inf'))
        )

        # Use softmax for better gradient flow
        similarity_scores = F.softmax(total_similarity, dim=1)

        # Get top k similar indices
        _, top_indices = torch.topk(similarity_scores, k=k, dim=1)

        # Gather results
        retrieved_cases = self.cases[top_indices]
        retrieved_r0s = self.r0_values[top_indices]
        retrieved_similarities = torch.gather(similarity_scores, 1, top_indices)

        return retrieved_cases, retrieved_r0s, retrieved_similarities

    # Add a method to monitor weights
    def get_current_weights(self):
        weights = F.softmax(torch.stack([self.case_sim_weight, self.pop_sim_weight]), dim=0)
        return {
            'case_weight': weights[0].item(),
            'pop_weight': weights[1].item(),
            'raw_case_weight': self.case_sim_weight.item(),
            'raw_pop_weight': self.pop_sim_weight.item()
        }

    def update_memory(self, x, population_size, disease_chars, r0s):
        """Update memory with new examples"""
        self.add_outbreaks_batch(x, population_size, disease_chars, r0s)

class DiseaseCharacteristicsEncoder(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        # Embeddings for each disease characteristic
        self.characteristic_embeddings = nn.ModuleDict({
            'waterborne': nn.Embedding(2, d_model // 4),
            'vectorborne': nn.Embedding(2, d_model // 4),
            'latent_period': nn.Embedding(2, d_model // 4),
            'waning_immunity': nn.Embedding(2, d_model // 4)
        })

        # Disease-specific attention masks
        self.attention_generators = nn.ModuleDict({
            'waterborne': nn.Sequential(
                nn.Linear(d_model // 4, d_model),
                nn.Sigmoid()
            ),
            'vectorborne': nn.Sequential(
                nn.Linear(d_model // 4, d_model),
                nn.Sigmoid()
            ),
            'latent_period': nn.Sequential(
                nn.Linear(d_model // 4, d_model),
                nn.Sigmoid()
            ),
            'waning_immunity': nn.Sequential(
                nn.Linear(d_model // 4, d_model),
                nn.Sigmoid()
            )
        })

        # Disease-specific feature transformations
        self.feature_transformers = nn.ModuleDict({
            'waterborne': nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.LayerNorm(d_model)
            ),
            'vectorborne': nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.LayerNorm(d_model)
            ),
            'latent_period': nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.LayerNorm(d_model)
            ),
            'waning_immunity': nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.LayerNorm(d_model)
            )
        })

        # Final combination layer
        self.feature_combiner = nn.Sequential(
            nn.Linear(d_model * 4, d_model),
            nn.GELU(),
            nn.LayerNorm(d_model)
        )

    def forward(self, x, disease_chars):
        # Split disease characteristics
        waterborne, vectorborne, latent, waning = disease_chars.unbind(-1)

        # Generate characteristic-specific embeddings and features
        disease_features = {}
        for name, char in zip(
            ['waterborne', 'vectorborne', 'latent_period', 'waning_immunity'],
            [waterborne, vectorborne, latent, waning]
        ):
            # Get embedding for this characteristic
            embedding = self.characteristic_embeddings[name](char.long())

            # Generate attention mask
            attention_mask = self.attention_generators[name](embedding)

            # Apply attention and transform
            attended = x * attention_mask.unsqueeze(1)
            transformed = self.feature_transformers[name](attended)
            disease_features[name] = transformed

        # Combine all disease-specific features
        combined = torch.cat(list(disease_features.values()), dim=-1)
        return self.feature_combiner(combined)

class RetrievalAugmentedR0Predictor(nn.Module):
    def __init__(self, base_model, memory_size=65536, initial_samples=65536):
        super().__init__()
        self.base_model = base_model
        self.outbreak_memory = OutbreakMemory(max_size=memory_size)

        # Register outbreak_memory as a submodule so its parameters are included in training
        self.add_module('outbreak_memory', self.outbreak_memory)

        # Add disease characteristics encoder
        self.disease_encoder = DiseaseCharacteristicsEncoder(d_model=256)  # Match base model d_model

        # Attention for combining processed retrieved features
        self.retrieval_attention = nn.MultiheadAttention(
            embed_dim=256,  # Match base model d_model
            num_heads=8,
            batch_first=True
        )

        # Project disease characteristics
        # self.disease_char_projection = nn.Sequential(
        #     nn.Linear(4, 64),
        #     nn.GELU(),
        #     nn.Linear(64, 256)
        # )

        # Modified final layer to incorporate retrieval context
        self.final_layer = nn.Sequential(
            nn.Linear(256 * 3 + 3, 256),  # Original features + retrieved context + growth/serial
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Linear(128, 3)  # [R0_lower, R0_mean, R0_upper]
        )

        # Ensure all components are on the same device as base_model
        device = next(base_model.parameters()).device
        self.to(device)
        print(f"RetrievalAugmentedR0Predictor initialized on device: {device}")

        # Pre-fill memory with initial samples
        self._initialize_memory(initial_samples)

    def _initialize_memory(self, num_samples):
        """Initialize memory with samples from the dataset"""
        print("Initializing outbreak memory...")
        dataset = DiseaseSimulationDataset(num_samples=num_samples)
        loader = DataLoader(
            dataset,
            batch_size=256,
            shuffle=True,
            num_workers=2,
            collate_fn=collate_padded_batch
        )

        for batch_data, days, r0s, _, _, disease_chars, populations in tqdm(loader):
            self.outbreak_memory.add_outbreaks_batch(
                batch_data,
                populations,
                disease_chars,
                r0s
            )

        print(f"Memory initialized with {self.outbreak_memory.current_size} samples")

    def process_sequence(self, x):
        """Process a sequence through the feature extraction pipeline"""
        # Normalize input
        x_normalized = self.base_model.input_normalizer(x)
        x = x_normalized.transpose(1, 2)
        x = self.base_model.initial_projection(x)
        x = x.transpose(1, 2)

        # CNN feature extraction
        cnn_features = []
        for cnn in self.base_model.cnn_layers:
            features = cnn(x)
            cnn_features.append(features)
            x = features

        x = torch.cat(cnn_features, dim=1)
        x = x.transpose(1, 2)
        x = self.base_model.cnn_projection(x)

        # Multi-scale temporal processing
        x = self.base_model.multi_scale(x.transpose(1, 2))
        x = x.transpose(1, 2)  # [batch, seq_len, features]

        return x

    # def forward(self, x, days_of_year=None, disease_chars=None, population_size=None):
    #     batch_size = x.size(0)
    #     device = x.device

    #     # Process input sequence
    #     x_processed = self.process_sequence(x)  # [batch, seq_len, d_model]
    #     seq_len = x_processed.size(1)
    #     d_model = x_processed.size(2)

    #     # Get similar cases
    #     retrieved_cases, retrieved_r0s, _ = self.outbreak_memory.get_similar_outbreaks_batch(
    #         x, population_size, disease_chars
    #     )  # [batch, k, seq_len], [batch, k]

    #     # Process each retrieved sequence
    #     k = retrieved_cases.size(1)

    #     # Ensure retrieved cases have the right sequence length
    #     if retrieved_cases.size(-1) != seq_len:
    #         retrieved_cases = F.pad(
    #             retrieved_cases,
    #             (0, seq_len - retrieved_cases.size(-1))
    #         ) if retrieved_cases.size(-1) < seq_len else retrieved_cases[:, :, :seq_len]

    #     # Process retrieved cases in chunks to avoid memory issues
    #     chunk_size = 32  # Process 32 sequences at a time
    #     retrieved_processed_list = []

    #     for i in range(0, batch_size * k, chunk_size):
    #         # Get chunk of retrieved cases
    #         end_idx = min(i + chunk_size, batch_size * k)
    #         chunk = retrieved_cases.view(-1, retrieved_cases.size(-1))[i:end_idx]

    #         # Process chunk
    #         processed_chunk = self.process_sequence(chunk)  # [chunk_size, seq_len, d_model]
    #         retrieved_processed_list.append(processed_chunk)

    #     # Concatenate all processed chunks
    #     retrieved_processed = torch.cat(retrieved_processed_list, dim=0)

    #     # Reshape to include k dimension
    #     retrieved_features = retrieved_processed.view(batch_size, k, seq_len, d_model)

    #     # Apply attention between query and retrieved sequences
    #     # Prepare attention inputs
    #     query = x_processed.unsqueeze(1)  # [batch, 1, seq_len, d_model]
    #     key = retrieved_features  # [batch, k, seq_len, d_model]

    #     # Reshape for attention
    #     query_flat = query.view(batch_size, seq_len, d_model)
    #     key_flat = key.view(batch_size, k * seq_len, d_model)

    #     # Apply attention
    #     attended_features, _ = self.retrieval_attention(
    #         query_flat,
    #         key_flat,
    #         key_flat,
    #         need_weights=False
    #     )  # [batch, seq_len, d_model]

    #     # Get global features
    #     global_features = self.base_model.attentive_pooling(x_processed)
    #     retrieval_features = self.base_model.attentive_pooling(attended_features)

    #     # Process disease characteristics
    #     disease_features = self.disease_char_projection(disease_chars)

    #     # Get growth rate and serial interval estimates
    #     growth_rate = self.base_model.growth_head(global_features)
    #     serial_interval = self.base_model.serial_head(global_features)

    #     # Combine all features
    #     combined = torch.cat([
    #         global_features,        # [batch, d_model]
    #         disease_features,       # [batch, d_model]
    #         retrieval_features,     # [batch, d_model]
    #         growth_rate,           # [batch, 1]
    #         serial_interval        # [batch, 1]
    #     ], dim=-1)

    #     # Get final predictions
    #     r0_estimates = self.final_layer(combined)
    #     r0_lower, r0_mean, r0_upper = r0_estimates.chunk(3, dim=-1)

    #     return r0_lower.squeeze(-1), r0_mean.squeeze(-1), r0_upper.squeeze(-1)

    def forward(self, x, days_of_year=None, disease_chars=None, population_size=None):
        batch_size = x.size(0)
        device = x.device

        # Process input sequence
        x_processed = self.process_sequence(x)  # [batch, seq_len, d_model]
        seq_len = x_processed.size(1)
        d_model = x_processed.size(2)

        # Get disease-specific features
        disease_features = self.disease_encoder(x_processed, disease_chars)

        # Get similar cases and their R0 values
        retrieved_cases, retrieved_r0s, similarities = self.outbreak_memory.get_similar_outbreaks_batch(
            x, population_size, disease_chars
        )  # [batch, k, seq_len], [batch, k], [batch, k]

        # Calculate weighted R0 prediction from retrieved cases
        weighted_r0_prediction = torch.sum(retrieved_r0s * similarities, dim=1)

        # Process each retrieved sequence
        k = retrieved_cases.size(1)

        # Ensure retrieved cases have the right sequence length
        if retrieved_cases.size(-1) != seq_len:
            retrieved_cases = F.pad(
                retrieved_cases,
                (0, seq_len - retrieved_cases.size(-1))
            ) if retrieved_cases.size(-1) < seq_len else retrieved_cases[:, :, :seq_len]

        # Process retrieved cases in chunks to avoid memory issues
        chunk_size = 32  # Process 32 sequences at a time
        retrieved_processed_list = []

        for i in range(0, batch_size * k, chunk_size):
            # Get chunk of retrieved cases
            end_idx = min(i + chunk_size, batch_size * k)
            chunk = retrieved_cases.view(-1, retrieved_cases.size(-1))[i:end_idx]

            # Process chunk
            processed_chunk = self.process_sequence(chunk)  # [chunk_size, seq_len, d_model]
            retrieved_processed_list.append(processed_chunk)

        # Concatenate all processed chunks
        retrieved_processed = torch.cat(retrieved_processed_list, dim=0)

        # Reshape to include k dimension
        retrieved_features = retrieved_processed.view(batch_size, k, seq_len, d_model)

        # Create R0 context embedding
        r0_context = torch.stack([
            retrieved_r0s,  # R0 values [batch, k]
            similarities,   # Similarity scores [batch, k]
        ], dim=-1)  # [batch, k, 2]

        # Create R0 context projection layer if it doesn't exist
        if not hasattr(self, 'r0_context_projection'):
            self.r0_context_projection = nn.Linear(2, d_model).to(device)

        # Project R0 context to d_model dimension
        r0_context_proj = self.r0_context_projection(r0_context)  # [batch, k, d_model]

        # Add R0 context to retrieved features
        retrieved_features = retrieved_features + r0_context_proj.unsqueeze(2)  # [batch, k, seq_len, d_model]

        # Apply attention between query and retrieved sequences
        query = x_processed.unsqueeze(1)  # [batch, 1, seq_len, d_model]
        key = retrieved_features  # [batch, k, seq_len, d_model]

        # Reshape for attention
        query_flat = query.view(batch_size, seq_len, d_model)
        key_flat = key.view(batch_size, k * seq_len, d_model)

        # Apply attention with disease context
        attended_features, _ = self.retrieval_attention(
            query_flat + disease_features,  # Add disease context to query
            key_flat,
            key_flat,
            need_weights=False
        )

        # Get global features
        global_features = self.base_model.attentive_pooling(x_processed)
        retrieval_features = self.base_model.attentive_pooling(attended_features)

        # Get disease features global representation
        disease_features_global = self.base_model.attentive_pooling(disease_features)  # [batch, d_model]

        # Get growth rate and serial interval estimates
        growth_rate = self.base_model.growth_head(global_features)
        serial_interval = self.base_model.serial_head(global_features)

        # Combine all features including disease-specific ones
        combined = torch.cat([
            global_features,        # [batch, d_model]
            disease_features_global,       # [batch, d_model]
            retrieval_features,     # [batch, d_model]
            growth_rate,           # [batch, 1]
            serial_interval,       # [batch, 1]
            weighted_r0_prediction.unsqueeze(-1)  # [batch, 1]
        ], dim=-1)

        # Get final predictions
        r0_estimates = self.final_layer(combined)
        r0_lower, r0_mean, r0_upper = r0_estimates.chunk(3, dim=-1)

        return r0_lower.squeeze(-1), r0_mean.squeeze(-1), r0_upper.squeeze(-1)

    def update_memory(self, x, population_size, disease_chars, r0s):
        """Update memory with new examples"""
        self.outbreak_memory.add_outbreaks_batch(x, population_size, disease_chars, r0s)

from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR
from torch.optim import AdamW
import pandas as pd
import numpy as np

def collate_padded_batch(batch):
    # Unpack the batch
    data_list = []
    days_list = []
    r0_list = []
    simulator_types = []
    params_list = []
    disease_chars_list = []  # New list for disease characteristics
    population_list = []

    for data, days, r0, sim_type, params, disease_chars, population in batch:  # Updated unpacking
        data_list.append(data)
        days_list.append(days)
        r0_list.append(r0)
        simulator_types.append(sim_type)
        params_list.append(params)
        disease_chars_list.append(disease_chars)  # Add disease characteristics
        population_list.append(population)

    # Stack tensors
    data = torch.stack(data_list)
    days = torch.stack(days_list)
    r0s = torch.stack(r0_list)
    disease_chars = torch.stack(disease_chars_list)  # Stack disease characteristics
    populations = torch.stack(population_list)

    return data, days, r0s, simulator_types, params_list, disease_chars, populations

def setup_drive_saving():
  """Setup Google Drive mounting and create checkpoint directory"""
  try:
      from google.colab import drive
      print("Mounting Google Drive...")
      drive.mount('/content/drive')
      drive_path = '/content/drive/MyDrive/r0_model_checkpoints'
      os.makedirs(drive_path, exist_ok=True)
      print(f"Created checkpoint directory at {drive_path}")
      return drive_path
  except:
      print("Could not mount Google Drive. Will save locally only.")
      return None

def train_large_scale(
    num_samples=3_000_000,
    batch_size=512,
    learning_rate=1e-4,
    warmup_fraction=0.1,  # First 10% of steps for warmup
    val_fraction=0.1,
    num_workers=2,
    checkpoint_every=10000,  # Save every 10k batches
    checkpoint_dir='checkpoints',
    device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
):
    # At start of training
    drive_path = setup_drive_saving()

    # Create checkpoint directories
    os.makedirs(checkpoint_dir, exist_ok=True)


    print("Initializing model...")

    memory_update_freq = 100000


    base_model = InfectiousDiseaseR0Extractor(
        seq_length=100,
        d_model=256,
        nhead=8,
        num_encoder_layers=6,
        num_cnn_layers=4,
        dim_feedforward=1024,
    ).to(device)
    model = RetrievalAugmentedR0Predictor(base_model)

    print(sum(p.numel() for p in model.parameters()))

    print("Generating dataset...")
    dataset = DiseaseSimulationDataset(num_samples=num_samples)

    # Split into train/val
    val_size = int(val_fraction * len(dataset))
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

    print("Creating dataloaders...")
    train_loader = DataLoader(
      train_dataset,
      batch_size=batch_size,
      shuffle=True,
      num_workers=2,  # Reduced number of workers
      pin_memory=True,
      collate_fn=collate_padded_batch
    )

    val_loader = DataLoader(
      val_dataset,
      batch_size=batch_size,
      shuffle=False,
      num_workers=2,  # Reduced number of workers
      pin_memory=True,
      collate_fn=collate_padded_batch
    )

    # Initialize optimizer
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    # Calculate number of warmup steps
    total_steps = len(train_loader)
    warmup_steps = int(total_steps * warmup_fraction)

    # Create scheduler chain: linear warmup followed by cosine annealing
    scheduler_warmup = LinearLR(
        optimizer,
        start_factor=0.1,
        end_factor=1.0,
        total_iters=warmup_steps
    )

    scheduler_cosine = CosineAnnealingLR(
        optimizer,
        T_max=total_steps - warmup_steps,
        eta_min=learning_rate/100  # Minimum LR will be 1% of initial LR
    )

    print(f"Starting training on {num_samples:,} samples...")
    model.train()
    running_loss = 0.0
    running_mpe = 0.0
    last_print = 0
    print_every = len(train_loader) // 100 if len(train_loader) > 10 else 1
    best_loss = float('inf')
    step = 0

    # Print initial weights before training
    # weights = model.outbreak_memory.get_current_weights()
    # print("\Initial similarity weights:")
    # print(f"Case weight: {weights['case_weight']:.3f}")
    # print(f"Population weight: {weights['population_weight']:.3f}")
    # print(f"Raw weights: {weights['raw_weights']}")

    for batch_idx, (batch_data, days, r0, simulator_types, params, disease_chars, populations) in enumerate(tqdm(train_loader)):
        # Move data to device
        batch_data = batch_data.to(device)
        days = days.to(device)
        r0 = r0.to(device)
        disease_chars = disease_chars.to(device)  # Move disease characteristics to device

        populations = populations.to(device)

        optimizer.zero_grad()
        r0_lower, r0_mean, r0_upper = model(
            batch_data,
            days_of_year=days,
            disease_chars=disease_chars,
            population_size=populations
        )



        # Calculate losses
        relative_error = torch.abs(r0_mean - r0) / r0
        prediction_loss = torch.mean(relative_error)

        lower_loss = torch.mean(0.05 * torch.max(r0 - r0_lower, torch.zeros_like(r0)) +
                              0.95 * torch.max(r0_lower - r0, torch.zeros_like(r0)))
        upper_loss = torch.mean(0.95 * torch.max(r0 - r0_upper, torch.zeros_like(r0)) +
                              0.05 * torch.max(r0_upper - r0, torch.zeros_like(r0)))

        loss = prediction_loss + lower_loss + upper_loss
        loss.backward()

        optimizer.step()

        # Step the appropriate scheduler
        if batch_idx < warmup_steps:
            scheduler_warmup.step()
        else:
            scheduler_cosine.step()

        step += 1

        # Save checkpoint periodically
        # In the checkpoint saving section:
        # if step % checkpoint_every == 0:
        #     # Local save
        #     checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{step}.pt')
        #     torch.save({
        #         'step': step,
        #         'model_state_dict': model.state_dict(),
        #         'optimizer_state_dict': optimizer.state_dict(),
        #         'scheduler_warmup_state_dict': scheduler_warmup.state_dict(),
        #         'scheduler_cosine_state_dict': scheduler_cosine.state_dict(),
        #         'loss': loss.item(),
        #         'learning_rate': optimizer.param_groups[0]['lr'],
        #     }, checkpoint_path)

        #     # Drive save if available
        #     if drive_path is not None:
        #         try:
        #             drive_checkpoint_path = os.path.join(drive_path, f'checkpoint_step_{step}.pt')
        #             torch.save({
        #                 'step': step,
        #                 'model_state_dict': model.state_dict(),
        #                 'optimizer_state_dict': optimizer.state_dict(),
        #                 'scheduler_warmup_state_dict': scheduler_warmup.state_dict(),
        #                 'scheduler_cosine_state_dict': scheduler_cosine.state_dict(),
        #                 'loss': loss.item(),
        #                 'learning_rate': optimizer.param_groups[0]['lr'],
        #             }, drive_checkpoint_path)
        #             print(f"\nSaved checkpoint to Drive: {drive_checkpoint_path}")
        #         except Exception as e:
        #             print(f"\nFailed to save to Drive: {str(e)}")

        # Calculate MPE
        mpe = torch.mean(torch.abs(r0_mean - r0) / r0 * 100)

        running_loss += loss.item()
        running_mpe += mpe.item()

        if (batch_idx) % print_every == 0:
            avg_loss = running_loss / (batch_idx - last_print + 1)
            avg_mpe = running_mpe / (batch_idx - last_print + 1)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"\nBatch {batch_idx+1}/{len(train_loader)}")
            print(f"Average Loss: {avg_loss:.4f}")
            print(f"Average MPE: {avg_mpe:.2f}%")
            print(f"Learning Rate: {current_lr:.2e}")
            last_print = batch_idx + 1
            running_loss = 0.0
            running_mpe = 0.0


            # weights = model.outbreak_memory.get_current_weights()
            # print("\nCurrent similarity weights:")
            # print(f"Case weight: {weights['case_weight']:.3f}")
            # print(f"Population weight: {weights['population_weight']:.3f}")
            # print(f"Raw weights: {weights['raw_weights']}")


    print("\nEvaluating on validation set...")
    model.eval()
    val_loss = 0.0
    val_mpe = 0.0

    with torch.no_grad():
        for batch_data, days, r0, simulator_types, params, disease_chars in tqdm(val_loader):
            batch_data = batch_data.to(device)
            days = days.to(device)
            r0 = r0.to(device)
            disease_chars = disease_chars.to(device)

            r0_lower, r0_mean, r0_upper = model(
                batch_data,
                days_of_year=days,
                disease_chars=disease_chars
            )

            loss = F.mse_loss(r0_mean, r0)
            mpe = torch.mean(torch.abs(r0_mean - r0) / r0 * 100)

            val_loss += loss.item()
            val_mpe += mpe.item()

    val_loss /= len(val_loader)
    val_mpe /= len(val_loader)

    print(f"\nValidation Loss: {val_loss:.4f}")
    print(f"Validation MPE: {val_mpe:.2f}%")

    # Save final model
    final_checkpoint_path = os.path.join(checkpoint_dir, 'final_model.pt')
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'final_loss': loss.item(),
        'learning_rate': optimizer.param_groups[0]['lr'],
    }, final_checkpoint_path)

    # Try to save to Drive
    try:
        drive_final_path = '/content/drive/MyDrive/r0_model_checkpoints/final_model.pt'
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'final_loss': loss.item(),
            'learning_rate': optimizer.param_groups[0]['lr'],
        }, drive_final_path)
    except:
        print("Could not save final model to Drive")

    return model

model = train_large_scale(num_samples=8500000)

